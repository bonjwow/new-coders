---
title: "The Impact of Educational Expenses of New Coders"
subtitle: "The Factors Affecting Education Costs of Beginner Programmers to Learn to Code aside from University Tuition"
author: "Bongju Yoo"
thanks: "https://github.com/bonjwow/new-coders"
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: ""
output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(janitor)
library(stargazer)
library(psych)
# install.packages("here")
library(here)
# install.packages("QuantPsyc")
library(QuantPsyc)
library(car)
library(kableExtra)
# install.packages("forecast")
library(forecast)
```


```{r, include=FALSE}
#### Custom functions ####

### Format number
# Source: https://stackoverflow.com/questions/29465941/format-number-in-r-with-both-comma-thousands-separator-and-specified-decimals

formatNumber <- function(numb, decimal) {
  return(format(round(as.numeric(numb), decimal), nsmall=decimal, big.mark=",")) 
}

### APA style correlation matrix 
# Source: https://stefaneng.github.io/apa_correlation_table/
# install.packages("Hmisc")
library(Hmisc)

apply_if <- function(mat, p, f) {
  # Fill NA with FALSE
  p[is.na(p)] <- FALSE
  mat[p] <- f(mat[p])
  mat
}

apaCorr <- function(mat, corrtype = "pearson") {
  matCorr <- mat
  if (class(matCorr) != "rcorr") {
    matCorr <- rcorr(mat, type = corrtype)
  }
  # Add one star for each p < 0.05, 0.01, 0.001
  stars <- apply_if(round(matCorr$r, 2), matCorr$P < 0.05, function(x) paste0(x, "*"))
  stars <- apply_if(stars, matCorr$P < 0.01, function(x) paste0(x, "*"))
  stars <- apply_if(stars, matCorr$P < 0.001, function(x) paste0(x, "*"))
  # Put - on diagonal and blank on upper diagonal
  stars[upper.tri(stars, diag = T)] <- "-"
  stars[upper.tri(stars, diag = F)] <- ""
  n <- length(stars[1,])
  colnames(stars) <- 1:n
  # Remove _ and convert to title case
  row.names(stars) <- tools::toTitleCase(sapply(row.names(stars), gsub, pattern="_", replacement = " "))
  # Add index number to row names
  row.names(stars) <- paste(paste0(1:n,"."), row.names(stars))
  stars
}
```


# Introduction

With an increasing interest in computer programming and the growth of coding related jobs, more and more people invest their time and money in taking programming courses [@citeHughes]. In this paper, I examine some of the factors that may affect educational expenses of new coders or entry-level programmers aside from university tuition, mainly focusing on the level of education, the size of city, commuting time, income, and learning time. Approximately 20,000 beginner programmers had participated in the survey [@citeSurvey2017].

Firstly, I processed correlation analysis to identify the relationship between each independent variable and dependent variable (education costs) pair and found that all the pairs have a positive linear relationship except for commuting time. Secondly, I used multiple linear regression to determine the statistical significance of the factors. The result of the analysis shows that commuting time is not statistically significant. Interestingly, education background is also not statistically significant associated with educational expenses of beginner programmers. Other variables, such as the size of city, income, and learning time, have a statistical significance on the costs.

This analysis shows that beginner programmers living in small towns and/or low-income households are likely to spend less money on learning to code aside from university tuition than those who live in bigger cities and/or high-income households. It is assumed that there is still a lack of educational resources for new coders living in rural areas and/or low-income households. Free online learning platforms such as freeCodeCamp can be a great resource of education for the coders.

The paper is organized as follows. The Data section describes features of the original survey data and how the data is preprocessed. The Model section explains the multiple linear regression model used to assess the association between each factor and educational costs of beginner programmers, and evaluates the model using its residual standard error, multiple R-squared, and F-statistic. The Results section summarises the results of the regression model and model evaluation processes in the Model section. Lastly, the Discussion section discusses the findings and potential limitations of the paper, and suggests directions for future research related to this data.

# Data

```{r table1, echo=FALSE, warning=FALSE, message=FALSE}
#### Get data ####
dfNewCoders <- 
  readr::read_csv(here::here("inputs/data/clean_new-coders.csv"))

```
## Data Collection
The survey was run by `freeCodeCamp`, a non-profit organization that helps people learn to code through their free online courses and build a network for their alumni [@citeAbout]. The purpose of the survey was to examine how the users learn to code [@cite2017Survey]. The survey was conducted over the internet, and the survey respondents were limited to those persons with less than 5 years learning programming; all the respondents were asked whether they have already been coding for more than 5 years or not before starting the survey [@cite2017Survey]. The survey is composed of 48 questions and takes about five minutes to complete, and the survey results are under the Open Data Common License, which can be freely distributed through the organization’ GitHub repository [@cite2017Survey].^[https://github.com/freeCodeCamp/2017-new-coder-survey]

## Description of Dataset
The original dataset used for this paper is obtained from `freeCodeCamp`’s GitHub page. The format of the dataset is a comma-separated values (CSV) file, which contains 136 columns and 18,175 observations. I selected 8 variables and cleaned the data using the R programming language [@citeR] and the `tidyverse` package [@cityTidyverse] and the `dplyr` package [@cityDplyr]. The selected variables are: Age, CityPopulation, CommuteTime, Gender,  Income, MoneyForLearning, MonthsProgramming, and SchoolDegree. The CityPopulation is the estimated number of city population of the recipient; the question asked was "About how many people live in your city?”, and there are three options to choose: “less than 100,000”, "between 100,000 and 1 million", and "more than 1 million". Since these answers were coded as strings, I converted them into numeric variables using the `recode` function of `dplyr` [@cityDplyr]. The answers for CommuteTime and SchoolDegree were also coded as strings in the original dataset, so I applied the same data cleaning process to the variables as I did with CityPopulation. Also, I omitted observations which have a missing value. Table \@ref(tab:tabViewDataset) is a partial view of the cleaned dataset after the preprocessing.


```{r tabViewDataset, echo=FALSE, warning=FALSE, message=FALSE}
#### A partial view of the cleaned dataset ####
tabCleanedData <-
  dfNewCoders[1:10,] %>%
  mutate_all(linebreak) %>%
  kable(caption = "A partial view of the cleaned dataset",
      booktabs = T,
      escape = F,
      "latex",
      col.names = linebreak(c("Gender", 
                              "Age",
                              "City\nPopulation",
                              "Commute\nTime",
                              "Income",
                              "Months\nProgramming",
                              "School\nDegree",
                              "Money For\nLearning"), align = "c")) %>%
  kable_classic(full_width = F)

tabCleanedData

```


Table 2 displays descriptive statistics for the cleaned data after the preprocessing. The total number of observations is 7,022 and the data type for all variables are numeric variables. The median age of the respondents is 30 years and the median income of them is about \$43,000 in U.S. dollars. The average of the respondents has spent about 2 years and about \$1,000 in U.S. dollars learning to code. 


```{r tabDescrStat, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
#### Descriptive statistics ####
stargazer::stargazer(data.frame(dfNewCoders), 
                     type = "latex", 
                     title = "Descriptive statistics for the cleaned dataset", 
                     header = FALSE,
                     single.row = TRUE)


```

## Correlation Analysis 

I ran a correlation matrix to check correlation coefficients between the variables. To create the correlation matrix, I used a custom function written by Stefan Eng [@citeEng]. His function contains the `Hmisc` package’s `rcorr` function to calculate the correlations with the p-values and display starts based on the significance level of each cell when the value is lower than a specific level [@citeHmisc]. Along with his custom to compute the correlation matrix, I used the `kable` package to print out the result in a table format [@citeKable]. As can be seen in Table \@ref(tab:tabCorrAnalysis), the result of the correlation matrix shows that all independent variables have a positive significant relationship with the dependent variable, MoneyForLearning, except for CommuteTime. This can be interpreted that a commuting time to work does not affect educational expenditure of new coders aside from their post-secondary education cost. 


```{r tabCorrAnalysis, echo=FALSE, warning=FALSE, message=FALSE}
#### Correlation analysis ####

### Print correlation coefficient
tblCorr <-
  dfNewCoders %>%
  as.matrix() %>%
  apaCorr() %>%
  data.frame() %>%
  kable(caption = "Correlation matrix for variables",
        booktabs = TRUE,
        col.names = c("1", "2", "3", "4", "5", "6", "7", "8")) %>%
  footnote(general = "* p < 0.05, ** p < 0.01, *** p < 0.001",
             general_title = "")

tblCorr


# No correlation coefficient > 0.9 
# No multicollinearity
```




# Model
```{r, include=FALSE}
#### TODO's ####
# Write out what each variable is
# Weaknesses and next steps
```

## Model Formulae

In the earlier section, I ran a correlation matrix to identify the correlation of each variable and the significance of the relationship between the dependent variable (MoneyForLearning) and other dependents variables. But, I was not able to estimate how much the independent variables impact the dependent variable from the correlation analysis. In this section, I will examine effects of the dependent variables on the dependent variable, focusing on one of the dependent variables: Income. The two main questions that I set out to answer in this section are: (1) How do beginner programmers' income affect the cost of their education to learn code? and (2) Will the effects change according to the population of the city where they live in. To obtain the answer for the first question, I used a simple linear regression model (Model 1) and multiple linear regression models (Model 2 and 3). All these three models use MoneyForLearning (the cost of their education to learn code) as a dependent variable. Model 1 contains only one independent variable, which is Income (see Model \@ref(eq:linRegModel1)); otherwise, Model 3 uses all the independent options except for CommuteTime: Gender, Age, CityPopulation, Income, MonthsProgramming, and SchoolDegree (see Model \@ref(eq:linRegModel3)). And, Model 2 uses the same independent variables, excluding Gender and Age. For the second question about the effects of the population of the city where the programmers reside, I used four simple linear regression models (see \@ref(eq:linRegModel2)). Like Model 1, all these models use MoneyForLearning as a dependent variable and Income as an independent variable (see Model \@ref(eq:linRegModel1)). But, the observations were grouped by the population size of the programmers. Model 4 contains any size of population; and Model 5,6, and 7 are the small (less than 100,000), medium (between 100,000 and 1 million), and large (more than 1 million) size population.


 
 
\begin{equation}
(\#eq:linRegModel1)
Y \sim \beta_0 + \beta_1X_1 + \epsilon
\end{equation}

\begin{equation}
(\#eq:linRegModel2)
Y \sim \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \epsilon
\end{equation}

\begin{equation}
(\#eq:linRegModel3)
Y \sim \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5 + \beta_6X_6 + \epsilon
\end{equation}


```{r, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
#### Multiple regression analysis ####

### Run linear regression models
multiReg1 <- lm(formula = 
               MoneyForLearning ~ 
               Income,
               data = dfNewCoders)

multiReg2 <- lm(formula = 
               MoneyForLearning ~ 
               # Gender +
               # Age +
               CityPopulation +
               # CommuteTime +
               Income +
               MonthsProgramming +
               SchoolDegree, 
               data = dfNewCoders)

multiReg3 <- lm(formula = 
               MoneyForLearning ~ 
               Gender +
               Age +
               CityPopulation +
               # CommuteTime +
               Income +
               MonthsProgramming +
               SchoolDegree, 
               data = dfNewCoders)


# summary(multiReg3)
stargazer::stargazer(multiReg1, multiReg2, multiReg3, 
          title = "Predictions of money for learning", 
          header = FALSE,
          type = "latex",
          column.labels = c("Model (1)", "Model (2)", "Model (3)"),
          model.numbers = FALSE)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
#### Multiple regression analysis (by city population) ####
dfSmCity <-
  dfNewCoders %>%
  filter(CityPopulation == 0)

dfMdCity <-
  dfNewCoders %>%
  filter(CityPopulation == 1)

dfLgCity <-
  dfNewCoders %>%
  filter(CityPopulation == 2)


### Run linear regression models
linReg1 <- lm(formula = 
               MoneyForLearning ~ 
               Income,
               data = dfNewCoders)

linReg2 <- lm(formula = 
               MoneyForLearning ~ 
               Income,
               data = dfSmCity)

linReg3 <- lm(formula = 
               MoneyForLearning ~ 
               Income,
               data = dfMdCity)

linReg4 <- lm(formula = 
               MoneyForLearning ~ 
               Income,
               data = dfLgCity)

stargazer::stargazer(linReg1, linReg2, linReg3, linReg4,
          title = "Predictions of money for learning by city population", 
          header = FALSE,
          type = "latex",
          column.labels = c("Model (4): Any", "Model (5): Small", "Model (6): Medium", "Model (7): Large"),
          model.numbers = FALSE,
          font.size = "tiny")


```



## Model Validation

Discuss model validation here...

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#### RMSE with custom function ####

# Source: https://medium.com/dev-genius/metrics-for-evaluating-linear-regression-models-36df305510d9
# Source: https://stackoverflow.com/questions/26237688/rmse-root-mean-square-deviation-calculation-in-r

### Define functions for R-squared, MSE, and RMSE
RSQ <- function(model) {
  result <- summary(model)$r.squared
  return(formatNumber(result, 3))
}

MSE <- function(error) {
  result <- mean(error^2)
  return(formatNumber(result, 3))  
}
    
RMSE <- function(error) {
  result <- sqrt(mean(error^2))
  return(formatNumber(result, 3)) 
}

```


```{r, echo=FALSE, warning=FALSE, message=FALSE}

### Calculate R-squared, MSE, and RMSE for each model 
vecModel <-
  c("Model (1)", "Model (2)", "Model (3)")

vecRSQ <- 
  c(RSQ(multiReg1), RSQ(multiReg2), RSQ(multiReg3))

vecMSE <-
  c(MSE(multiReg1$residuals), MSE(multiReg2$residuals), MSE(multiReg3$residuals))

vecRMSE <-
  c(RMSE(multiReg1$residuals), RMSE(multiReg2$residuals), RMSE(multiReg3$residuals))

### Display the result in a table
dfModelValid <-
  data.frame(vecModel, vecRSQ, vecMSE, vecRMSE) %>%
  kable(caption = "Results of model validation metrics for model group 1",
        booktabs = TRUE,
        col.names = c("Model", "R-squared", "MSE", "RMSE")) %>%
  column_spec(column = 1, width = "4cm") %>%
  column_spec(column = 2:4, width = "3cm")

dfModelValid

# If R-squared is closer to 0 = Good!
# Model3 has the lowest RMSE among the 3 models => Model3 is the best!
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

### Calculate R-squared, MSE, and RMSE for each model
vecModel_city <-
  c("Model (4)", "Model (5)", "Model (6)", "Model (7)")

vecRSQ_city <- 
  c(RSQ(linReg1), RSQ(linReg2), RSQ(linReg3), RSQ(linReg4))

vecMSE_city <-
  c(MSE(linReg1$residuals), MSE(linReg2$residuals), MSE(linReg3$residuals),  MSE(linReg4$residuals))

vecRMSE_city <-
  c(RMSE(linReg1$residuals), RMSE(linReg2$residuals), RMSE(linReg3$residuals), RMSE(linReg4$residuals))

### Display the result in a table
dfModelValidCity <-
  data.frame(vecModel_city, vecRSQ_city, vecMSE_city, vecRMSE_city) %>%
  kable(caption = "Results of model validation metrics for model group 2",
        booktabs = TRUE,
        col.names = c("Model", "R-squared", "MSE", "RMSE")) %>%
  column_spec(column = 1, width = "4cm") %>%
  column_spec(column = 2:4, width = "3cm")

dfModelValidCity

```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
#### Multicollinearity diagnostics ####
# Check multicollinearity with Variance Inflation Factor (VIF) & Tolerance

multicolDiagno <- function(mode, title) {
  betaModel2 <- QuantPsyc::lm.beta(mode)
  vifModel2 <- car::vif(mode)
  tolModel2 <- 1/car::vif(mode)
  
  dfMulticol <-
    betaModel2 %>%
    rbind(formatNumber(unname(betaModel2), 3)) %>%
    rbind(formatNumber(unname(vifModel2), 3)) %>%
    rbind(formatNumber(unname(unname(tolModel2)), 3)) %>%
    cbind(c("B", "Beta", "VIF", "Tolerance"))
    # dplyr::relocate(Metric, .before = CityPopulation) %>%
    # dplyr::select(Metric, everything()) %>%
    # kable()
  
  ### Relocate 'Metric' column: alternative to 'dplyr::relocate()'
  dfMulticol <- dfMulticol[-1,]
  dfMulticol <- dfMulticol[,c(ncol(dfMulticol),1:ncol(dfMulticol)-1)]
  
  kable(dfMulticol, 
        caption = title,
        booktabs = TRUE)
}
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#### Multicollinearity diagnostics for Model (2) ####
multicolDiagno(multiReg2, "Results of multicollinearity diagnostic metrics for Model (2)")
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
#### Multicollinearity diagnostics for Model (3) ####
multicolDiagno(multiReg3, "Results of multicollinearity diagnostic metrics for Model (3)")
```



# Results
```{r, include=FALSE}
#### Beta ####
# TODO: create a bar plot to show which variable affects the most with Beta
```




# Discussion
```{r, include=FALSE}
#### TODO's ####
# Write at least 2.5 pages for the Discussion section
# Weaknesses and next steps
```


\newpage

# Appendix {-}

\newpage


# References


