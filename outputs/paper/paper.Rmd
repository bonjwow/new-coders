---
title: "What Makes New Coders Spend More Money"
subtitle: "The Factors Affecting Beginner Programmers' Expenses to Learn to Code"
author: "Bongju Yoo"
thanks: "Code and data are available at: [https://github.com/bonjwow/new-coders]"
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: "I examine potential factors that may affect education costs of new coders to learn to code aside from their post secondary education based on the responses of the online survey run by freeCodeCamp in 2017. I use a correlation matrix and linear regression models to see if there is a correlation and causal relationship between the potential factors and the costs of learning to code. The results of the analysis show that a commuting time to work and the level of education do not affect educational expenditure of new coders to learn to code. It is also found that there is a correlation between each of the independent variables and the dependent variable, excluding a commuting time; however, there is no significant causal relationship between the variables. \\par \\textbf{Keywords:} Programmers, Coder, Education Costs, Coding, Online education, Open education"
output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(janitor)
library(stargazer)
library(psych)
# install.packages("here")
library(here)
# install.packages("QuantPsyc")
library(QuantPsyc)
library(car)
library(kableExtra)
# install.packages("forecast")
library(forecast)
```


```{r, include=FALSE}
#### Custom functions ####

### Format number
# Source: https://stackoverflow.com/questions/29465941/format-number-in-r-with-both-comma-thousands-separator-and-specified-decimals

formatNumber <- function(numb, decimal) {
  return(format(round(as.numeric(numb), decimal), nsmall=decimal, big.mark=",")) 
}

### APA style correlation matrix 
# Source: https://stefaneng.github.io/apa_correlation_table/
# install.packages("Hmisc")
library(Hmisc)

apply_if <- function(mat, p, f) {
  # Fill NA with FALSE
  p[is.na(p)] <- FALSE
  mat[p] <- f(mat[p])
  mat
}

apaCorr <- function(mat, corrtype = "pearson") {
  matCorr <- mat
  if (class(matCorr) != "rcorr") {
    matCorr <- rcorr(mat, type = corrtype)
  }
  # Add one star for each p < 0.05, 0.01, 0.001
  stars <- apply_if(round(matCorr$r, 2), matCorr$P < 0.05, function(x) paste0(x, "*"))
  stars <- apply_if(stars, matCorr$P < 0.01, function(x) paste0(x, "*"))
  stars <- apply_if(stars, matCorr$P < 0.001, function(x) paste0(x, "*"))
  # Put - on diagonal and blank on upper diagonal
  stars[upper.tri(stars, diag = T)] <- "-"
  stars[upper.tri(stars, diag = F)] <- ""
  n <- length(stars[1,])
  colnames(stars) <- 1:n
  # Remove _ and convert to title case
  row.names(stars) <- tools::toTitleCase(sapply(row.names(stars), gsub, pattern="_", replacement = " "))
  # Add index number to row names
  row.names(stars) <- paste(paste0(1:n,"."), row.names(stars))
  stars
}
```


# Introduction

With an increasing interest in computer programming and the growth of coding related jobs, more and more people invest their time and money in taking programming courses [@citeHughes]. In this paper, I examine some of the factors that may affect educational expenses of new coders or entry-level programmers aside from university tuition, mainly focusing on the level of education, the size of city, commuting time, income, and learning time. Approximately 20,000 beginner programmers had participated in the survey [@citeSurvey2017].

Firstly, I processed correlation analysis to identify the relationship between each independent variable and dependent variable (the costs of learning to code) pair and found that all the pairs have a positive linear relationship except for a commuting time. Secondly, I used a combination of simple linear regression models and multiple linear regressions to determine the statistical significance of the factors. The result of the analysis shows that the level of education is not statistically significant on the new coders’ expenses to learn to code. Also, the analysis shows that the income of the programmers is statistically significant on the costs. So, it can be argued that those beginner programmers living in small towns and/or low-income households are likely to spend less money on learning to code aside from college or university tuition than those who live in bigger cities and/or high-income households. It is assumed that there is still a lack of educational resources for new coders living in rural areas and/or low-income households. Free online learning platforms such as freeCodeCamp can be a great resource of education for the coders. Unfortunately, all the regression models used to examine the effect of new coders' income had an extremely low R-squared score. So, it fails to compute the average of predicted costs of the programmers with specific values of the independent variable(s).

The paper is organized as follows. The Data section describes features of the original survey data and how the data is preprocessed. The Model section explains the multiple linear regression model used to assess the association between each factor and educational costs of beginner programmers, and evaluates the model using its residual standard error, multiple R-squared, and F-statistic. The Results section summarises the results of the regression model and model evaluation processes in the Model section. Lastly, the Discussion section discusses the findings and potential limitations of the paper, and suggests directions for future research related to this data.

# Data

```{r table1, echo=FALSE, warning=FALSE, message=FALSE}
#### Get data ####
dfNewCoders <- 
  readr::read_csv(here::here("inputs/data/clean_new-coders.csv"))

```
## Data Collection
The survey was run by `freeCodeCamp`, a non-profit organization that helps people learn to code through their free online courses and build a network for their alumni [@citeAbout]. The purpose of the survey was to examine how the users learn to code [@cite2017Survey]. The survey was conducted over the internet, and the survey respondents were limited to those persons with less than 5 years learning programming; all the respondents were asked whether they have already been coding for more than 5 years or not before starting the survey [@cite2017Survey]. The survey is composed of 48 questions and takes about five minutes to complete, and the survey results are under the Open Data Common License, which can be freely distributed through the organization’ GitHub repository [@cite2017Survey].^[https://github.com/freeCodeCamp/2017-new-coder-survey]

## Description of Dataset
The original dataset used for this paper is obtained from `freeCodeCamp`’s GitHub page. The format of the dataset is a comma-separated values (CSV) file, which contains 136 columns and 18,175 observations. I selected 8 variables and cleaned the data using the R programming language [@citeR] and the `tidyverse` package [@cityTidyverse] and the `dplyr` package [@cityDplyr]. The selected variables are: Age, CityPopulation, CommuteTime, Gender,  Income, MoneyForLearning, MonthsProgramming, and SchoolDegree. The CityPopulation is the estimated number of city population of the recipient; the question asked was "About how many people live in your city?”, and there are three options to choose: “less than 100,000”, "between 100,000 and 1 million", and "more than 1 million". Since these answers were coded as strings, I converted them into numeric variables using the `recode` function of `dplyr` [@cityDplyr]. The answers for CommuteTime and SchoolDegree were also coded as strings in the original dataset, so I applied the same data cleaning process to the variables as I did with CityPopulation. Also, I omitted observations which have a missing value. Table \@ref(tab:tabViewDataset) is a partial view of the cleaned dataset after the preprocessing.


```{r tabViewDataset, echo=FALSE, warning=FALSE, message=FALSE}
#### A partial view of the cleaned dataset ####
tabCleanedData <-
  dfNewCoders[1:10,] %>%
  mutate_all(linebreak) %>%
  kable(caption = "A partial view of the cleaned dataset",
      booktabs = T,
      escape = F,
      "latex",
      col.names = linebreak(c("Gender", 
                              "Age",
                              "City\nPopulation",
                              "Commute\nTime",
                              "Income",
                              "Months\nProgramming",
                              "School\nDegree",
                              "Money For\nLearning"), align = "c")) %>%
  kable_classic(full_width = F)

tabCleanedData

```


Table 2 displays descriptive statistics for the cleaned data after the preprocessing. The total number of observations is 7,022 and the data type for all variables are numeric variables. The median age of the respondents is 30 years and the median income of them is about \$43,000 in U.S. dollars. The average of the respondents has spent about 2 years and about \$1,000 in U.S. dollars learning to code. 


```{r tabDescrStat, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
#### Descriptive statistics ####
stargazer::stargazer(data.frame(dfNewCoders), 
                     type = "latex", 
                     title = "Descriptive statistics for the cleaned dataset", 
                     header = FALSE,
                     single.row = TRUE)


```

## Correlation Analysis 

I ran a correlation matrix to check correlation coefficients between the variables. To create the correlation matrix, I used a custom function written by Stefan Eng [@citeEng]. His function contains the `Hmisc` package’s `rcorr` function to calculate the correlations with the p-values and display starts based on the significance level of each cell when the value is lower than a specific level [@citeHmisc]. Along with his custom to compute the correlation matrix, I used the `kable` package to print out the result in a table format [@citeKable]. As can be seen in Table \@ref(tab:tabCorrAnalysis), the result of the correlation matrix shows that all independent variables have a positive significant relationship with the dependent variable, MoneyForLearning, except for CommuteTime. This can be interpreted that a commuting time to work does not affect educational expenditure of new coders aside from their post-secondary education cost. 


```{r tabCorrAnalysis, echo=FALSE, warning=FALSE, message=FALSE}
#### Correlation analysis ####

### Print correlation coefficient
tblCorr <-
  dfNewCoders %>%
  as.matrix() %>%
  apaCorr() %>%
  data.frame() %>%
  kable(caption = "Correlation matrix for variables",
        booktabs = TRUE,
        col.names = c("1", "2", "3", "4", "5", "6", "7", "8")) %>%
  footnote(general = "* p < 0.05, ** p < 0.01, *** p < 0.001",
             general_title = "")

tblCorr


# No correlation coefficient > 0.9 
# No multicollinearity
```




# Model
```{r, include=FALSE}
#### TODO's ####
# Write out what each variable is
# Weaknesses and next steps
```

## Model Formulae

In the earlier section, I ran a correlation matrix to identify the correlation of each variable and the significance of the relationship between the dependent variable (MoneyForLearning) and other dependents variables. But, I was not able to estimate how much the independent variables impact the dependent variable from the correlation analysis. In this section, I will examine effects of the dependent variables on the dependent variable, focusing on one of the dependent variables: Income. The two main questions that I set out to answer in this section are: (1) How do beginner programmers' income affect the cost of their education to learn code? and (2) Will the effects change according to the population of the city where they live in. 

To obtain the answer for the first question, I used a simple linear regression model (Equation \@ref(eq:linRegModel1)) and multiple linear regression models (Equation \@ref(eq:linRegModel2) and \@ref(eq:linRegModel3)). All these three models use MoneyForLearning (the cost of their education to learn code) as a dependent variable. Model 1 contains only one independent variable, which is Income (see Equation \@ref(eq:linRegModel1)); otherwise, Model 3 uses all the independent options except for CommuteTime: Gender, Age, CityPopulation, Income, MonthsProgramming, and SchoolDegree (see Equation \@ref(eq:linRegModel3)). As mentioned in the Correlation Analysis section, the CommuteTime (a commuting time) variable does not significantly affect the response variable, so it is excluded from the regression models. And, Model 2 uses the same independent variables, excluding Gender and Age. For the second question about the effects of the population of the city where the programmers reside, I used four simple linear regression models (see Equation \@ref(eq:linRegModel2)). Like Model 1, all these models use MoneyForLearning as a dependent variable and Income as an independent variable (see Equation \@ref(eq:linRegModel1)). But, the observations were grouped by the population size of the programmers. Model 4 contains any size of population; and Model 5,6, and 7 are the small (less than 100,000), medium (between 100,000 and 1 million), and large (more than 1 million) size population.

 
\begin{equation}
(\#eq:linRegModel1)
Y \sim \beta_0 + \beta_1X_1 + \epsilon
\end{equation}

\begin{equation}
(\#eq:linRegModel2)
Y \sim \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \epsilon
\end{equation}

\begin{equation}
(\#eq:linRegModel3)
Y \sim \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5 + \beta_6X_6 + \epsilon
\end{equation}

## Data Preprocessing

As explained in the Data section, categorical variables were converted to continuous variables for the linear regression models. For example, the SchoolDegree variable (the level of education) was a categorical variable with character values. So, I converted the variable into a variable with numeric values, using the `dplyr` package. A complete list of survey questions and response options for each question can be found in the Appendix. Since the MoneyForLearning variable (the level of education) was not only categorical but also ordered and ranked in the original dataset, I converted the variable into a variable with numeric values. Also, I merged some of the response options of the variable. For instance, it is somewhat hard to say that a high school diploma or equivalent (GED) is higher than trade, technical, or vocational training, and a professional degree, such as MBA, MD, or JD, is higher than a non-professional master's degree. So, I merged the options, using the `recode` function of the `dplyr` package [@cityDplyr].

## Model Validation

I will use Mean Square Error (MSE) and Root Mean Square Error (RMSE) to evaluate the accuracy of the model. As the name implies, MSE is defined as the mean of the square of the residuals obtained from a regression model, and RMSE is the square root of MSE [@citeHolmes]. I will use the residuals which are the results of the `lm` function to calculate MSE and RMSE of the models. Firstly, I will calculate the square of the residuals, and then computed the mean of the squared value for MSE (see Equation \@ref(eq:mse)). Secondly, for RMSE, I will use the `sqrt` function to calculate the square root of MSE (see Equation \@ref(eq:rmse)). Once I obtain the results of MSE and RMSE, I will compare them to see how accurately the models predict the response.

\begin{equation}
(\#eq:mse)
MSE = \frac{1}{N}\sum_{i=1}^{N} (residual_{i})^2
\end{equation}

\begin{equation}
(\#eq:rmse)
RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^{N} (residual_{i})^2}
\end{equation}

Since two multiple linear regression models (Model 2 and 3) are used in the first group of regression models, I will check if multicollinearity exists in these regression models, using variance inflation factor (VIF) and tolerance. I will use the `car` package to calculate the values of VIF and tolerance for each model [@citeCar]. Also, I will identify how much each of the independent variables affects the dependent variable with the standardized regression coefficients (beta). the `QuantPsyc` package will be used to compute the beta value [@citeQuanPsy].

# Results

## Results of Regression Model

All the regression models in the first group of the models had p-values of F-statistic lower than a significance level of 0.05 (see Table 4). And, each of the independent variables also had a p-value of less than 0.05 except for the SchoolDegree variable in Model 3. This can be interpreted that all the independent variables excluding the SchoolDegree variable have an impact on the response variable (MoneyForLearning), and the level of education (SchoolDegree) is not statistically significant on the response variable with holding all other variables. However, the SchoolDegree variable becomes statistically significant if the Gender and Age variables are excluded from the regression model.

```{r tabLinRegModel1, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
#### Linear regression models (Group 1) ####

### Run linear regression models
multiReg1 <- lm(formula = 
               MoneyForLearning ~ 
               Income,
               data = dfNewCoders)

multiReg2 <- lm(formula = 
               MoneyForLearning ~ 
               # Gender +
               # Age +
               CityPopulation +
               # CommuteTime +
               Income +
               MonthsProgramming +
               SchoolDegree, 
               data = dfNewCoders)

multiReg3 <- lm(formula = 
               MoneyForLearning ~ 
               Gender +
               Age +
               CityPopulation +
               # CommuteTime +
               Income +
               MonthsProgramming +
               SchoolDegree, 
               data = dfNewCoders)


# summary(multiReg3)
stargazer::stargazer(multiReg1, multiReg2, multiReg3, 
          title = "Predictions of money for learning", 
          header = FALSE,
          type = "latex",
          column.labels = c("Model (1)", "Model (2)", "Model (3)"),
          model.numbers = FALSE)
```

The second group of regression models to examine the effects of the population of the city where the programmers reside shows that all the models have statistically significant p-values of F-statistic except for Model 5, the model for the small city with less than 100,000 residents (see Table 5). In other words, beginner programmers' income affects the cost of their education to learn code; however, their income does not affect the cost when the model contains only observations from the small city.

```{r tabLinRegModel2, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
#### Linear regression models (Group 2) ####
dfSmCity <-
  dfNewCoders %>%
  filter(CityPopulation == 0)

dfMdCity <-
  dfNewCoders %>%
  filter(CityPopulation == 1)

dfLgCity <-
  dfNewCoders %>%
  filter(CityPopulation == 2)


### Run linear regression models
linReg1 <- lm(formula = 
               MoneyForLearning ~ 
               Income,
               data = dfNewCoders)

linReg2 <- lm(formula = 
               MoneyForLearning ~ 
               Income,
               data = dfSmCity)

linReg3 <- lm(formula = 
               MoneyForLearning ~ 
               Income,
               data = dfMdCity)

linReg4 <- lm(formula = 
               MoneyForLearning ~ 
               Income,
               data = dfLgCity)

stargazer::stargazer(linReg1, linReg2, linReg3, linReg4,
          title = "Predictions of money for learning by city population", 
          header = FALSE,
          type = "latex",
          column.labels = c("Model (4): Any", "Model (5): Small", "Model (6): Medium", "Model (7): Large"),
          model.numbers = FALSE,
          font.size = "tiny")


```


## Results of Model Validation

Table \@ref(tab:tab6) is the results of MSE and RMSE of the first group consisting of Model 1, 2, and 3. Among these three models, Model 3 containing all independent variable options has the lowest value of RMSE. So, it can be said that Model 3 predicts the response variable more accurately with holding all independent variables, compared to Model 1 and 2 which contains the Income variable or excludes the Age and Gender variables.

Table \@ref(tab:tab7) is the results of MSE and RMSE of the second group consisting of Model 4, 5, 6, and 7. Among these four models, Model 5 (the regression model for the small city) has the lowest MSE and RMSE. However, as explained earlier, the p-values of F-statistic for the model was higher than a significance level of 0.05, which means the model is not statistically significant. The most accurate regression model excluding Model 5 among the four models was Model 4 which includes cities of all sizes, followed by Model 7. 

Model 3 and 4 are the most accurate model in each of the groups. However, as can be seen in Table \@ref(tab:tab6) and \@ref(tab:tab7), all the models have an extremely low R-squared value, which is very close to 0. In other words, the accuracy of the regression models is intensively low because the residuals (the difference between the measured values and the predicted values of the models) are high. Thus, it is hard to say that those six variables (Gender, Age, CityPopulation, Income, MonthsProgramming, and SchoolDegree) have a significant effect on education costs of beginner programmers to learn to code.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#### RMSE with custom function ####

# Source: https://medium.com/dev-genius/metrics-for-evaluating-linear-regression-models-36df305510d9
# Source: https://stackoverflow.com/questions/26237688/rmse-root-mean-square-deviation-calculation-in-r

### Define functions for R-squared, MSE, and RMSE
RSQ <- function(model) {
  result <- summary(model)$r.squared
  return(formatNumber(result, 3))
}

MSE <- function(error) {
  result <- mean(error^2)
  return(formatNumber(result, 3))  
}
    
RMSE <- function(error) {
  result <- sqrt(mean(error^2))
  return(formatNumber(result, 3)) 
}

```

```{r tab6, echo=FALSE, warning=FALSE, message=FALSE}

### Calculate R-squared, MSE, and RMSE for each model 
vecModel <-
  c("Model (1)", "Model (2)", "Model (3)")

vecRSQ <- 
  c(RSQ(multiReg1), RSQ(multiReg2), RSQ(multiReg3))

vecMSE <-
  c(MSE(multiReg1$residuals), MSE(multiReg2$residuals), MSE(multiReg3$residuals))

vecRMSE <-
  c(RMSE(multiReg1$residuals), RMSE(multiReg2$residuals), RMSE(multiReg3$residuals))

### Display the result in a table
dfModelValid <-
  data.frame(vecModel, vecRSQ, vecMSE, vecRMSE) %>%
  kable(caption = "Results of model validation metrics for model group 1",
        booktabs = TRUE,
        col.names = c("Model", "R-squared", "MSE", "RMSE")) %>%
  column_spec(column = 1, width = "4cm") %>%
  column_spec(column = 2:4, width = "3cm")

dfModelValid
```

```{r tab7, echo=FALSE, warning=FALSE, message=FALSE}

### Calculate R-squared, MSE, and RMSE for each model
vecModel_city <-
  c("Model (4)", "Model (5)", "Model (6)", "Model (7)")

vecRSQ_city <- 
  c(RSQ(linReg1), RSQ(linReg2), RSQ(linReg3), RSQ(linReg4))

vecMSE_city <-
  c(MSE(linReg1$residuals), MSE(linReg2$residuals), MSE(linReg3$residuals),  MSE(linReg4$residuals))

vecRMSE_city <-
  c(RMSE(linReg1$residuals), RMSE(linReg2$residuals), RMSE(linReg3$residuals), RMSE(linReg4$residuals))

### Display the result in a table
dfModelValidCity <-
  data.frame(vecModel_city, vecRSQ_city, vecMSE_city, vecRMSE_city) %>%
  kable(caption = "Results of model validation metrics for model group 2",
        booktabs = TRUE,
        col.names = c("Model", "R-squared", "MSE", "RMSE")) %>%
  column_spec(column = 1, width = "4cm") %>%
  column_spec(column = 2:4, width = "3cm")

dfModelValidCity

```


Table \@ref(tab:tab8) is the results of the multicollinearity diagnostic test for Model (2). The results show that the MonthsProgramming variable has the highest beta value, which means the independent variable has the most significant effect on the dependent variable (MoneyForLearning) among those four independent variables included in the regression model. However, all the beta values of the independent variables are extremely low and close to 0, meaning that their effects are not significant. All the VIF and tolerance of each variable are very close to 1, which means there is almost no multicollinearity; a VIF of 10 or higher, or tolerance close 0 indicates that the model might have multicollinearity [@citeWilliams]. 

Table \@ref(tab:tab9) is the results of the multicollinearity diagnostic test for Model (3). The variable which has the highest beta value is the CityPopulation variable, which means the size of the city has the most significant effect on education costs of beginner programmers to learn to code among those six independent variables used in the regression model. Like the results of the test for Model (2), Model (3) also does not have significant multicollinearity; the VIF and tolerance of each variable of the model are in an acceptable range. In concussion, there is no significant multicollinearity found in both Model (2) and (3). However, all the regression models have an extremely low R-squared value, meaning the regression models have poor accuracy of prediction. 


```{r, echo=FALSE, warning=FALSE, message=FALSE}
#### Multicollinearity diagnostics ####
# Check multicollinearity with Variance Inflation Factor (VIF) & Tolerance

multicolDiagno <- function(mode, title) {
  betaModel2 <- QuantPsyc::lm.beta(mode)
  vifModel2 <- car::vif(mode)
  tolModel2 <- 1/car::vif(mode)
  
  dfMulticol <-
    betaModel2 %>%
    rbind(formatNumber(unname(betaModel2), 3)) %>%
    rbind(formatNumber(unname(vifModel2), 3)) %>%
    rbind(formatNumber(unname(unname(tolModel2)), 3)) %>%
    cbind(c("B", "Beta", "VIF", "Tolerance"))
    # dplyr::relocate(Metric, .before = CityPopulation) %>%
    # dplyr::select(Metric, everything()) %>%
    # kable()
  
  ### Relocate 'Metric' column: alternative to 'dplyr::relocate()'
  dfMulticol <- dfMulticol[-1,]
  dfMulticol <- dfMulticol[,c(ncol(dfMulticol),1:ncol(dfMulticol)-1)]
  
  kable(dfMulticol, 
        caption = title,
        booktabs = TRUE)
}
```

```{r tab8, echo=FALSE, warning=FALSE, message=FALSE}
#### Multicollinearity diagnostics for Model (2) ####
multicolDiagno(multiReg2, "Results of multicollinearity diagnostic metrics for Model (2)")
```


```{r tab9, echo=FALSE, warning=FALSE, message=FALSE}
#### Multicollinearity diagnostics for Model (3) ####
multicolDiagno(multiReg3, "Results of multicollinearity diagnostic metrics for Model (3)")
```

\newpage

# Discussion

## Summary
In this paper, I examine potential factors that may affect costs of beginner programmers to learn to code aside from post secondary education based on the online survey run by freeCodeCamp in 2017. Firstly, I chose eight variables, including the dependent variable (educational expenses of the programmers) from the original dataset. Secondly, I ran a correlation matrix for all the independent variables to examine how closely they are related with the dependent variable. Thirdly, I used linear regression models to determine influence factors on the dependent variable from the independent variables excluding a commuting time which was confirmed not to have the correlation with the dependent variable. Fourthly, I ran two groups of linear regression models to examine effects of the dependent variables on the dependent variable: the first group was for identifying how programmers’ income affects their educational costs to learn to code and the second group was for looking into how the effect of their income changes according to the population of the city where the programmers live in. Finally, I evaluated the models using R-squared, MSE and RMSE scores of each model. Also, I used beta, VIF, and tolerance scores to check if there is multicollinearity in the multiple linear regression models. 

Summary of the findings are as follows: (1) The statistical relationship exists between all the independent variables excluding a commuting time and new coders’ expenses to learn to code aside from their college or university tuition, (2) The level of education is not statistically significant on the new coders’ expenses to learn to code, (3) Their income is statistically significant on the costs; but, the prediction model can become less accurate if the model includes only response data of the programmers from the small city, (4) There is no multicollinearity found in the multiple linear regression models, and lastly (5) All the models used in this paper have an extremely low R-squared score, which means that it is difficult to argue that those selected independent variable significantly affect the independent variable. In short, it can be argued that there is a correlation between each of the independent variables (the level of education, the size of city, a commuting time, income, and learning time) and the dependent variable (the cost of learning to code); however, there is no significant causal relationship between the variables.

## Casuality
In addition to the low R-squared scores, there is another reason that causality cannot be explained from the regression models. In reality, there will be plenty of factors that actually affect beginner programmers’ expenses to learn to code besides those selected seven variables. For example, there might be some institutions where the programmers can learn to code near their home, or some of them might be heavily exposed to online advertisements of some online-learning platforms that teach programming. In other words, realistically, it is almost impossible to measure all the variables happening in the large world and include them in the model. However, we can always measure which variables are significant and determine the most appropriate variables to make a model closer to the reality. For future studies, I suggest running the same analysis process, from the correlation analysis to linear regression models and model validation, with other variables that are not covered in this paper. Also, surveys of other years can be used to compare the results of this analysis.

\newpage

# Appendix {-}
## Survey questions {-}

Q. [Age] How old are you?

* Type: integer

Q. [CityPopulation] About how many people live in your city?

* Type: string

* Options:
  1. less than 100,000
  2. between 100,000 and 1 million
  3. more than 1 million

Q. [CommuteTime] About how many minutes does it take you to get to work each day?

* Type: string

* Options:
  1. I work from home
  2. Less than 15 minutes
  3. 15 to 29 minutes
  4. 30 to 44 minutes
  5. 45 to 60 minutes
  6. More than 60 minutes

Q. [Gender] What's your gender?

* Type: string

* Options:
  1. male
  2. female

Q. [Income] About how much money did you make last year, in US dollars?

* Type: integer

Q. [MoneyForLearning] Aside from university tuition, about how much money have you spent on learning to code so far, in US dollars?

* Type: integer

Q. [MonthsProgramming] About how many months have you been programming for?

* Type: integer

Q. [SchoolDegree] What's the highest degree or level of school you have completed?

* Type: string

* Options:
  1. no high school (secondary school)
  2. some high school
  3. high school diploma or equivalent (GED)
  4. trade, technical, or vocational training
  5. some college credit, no degree
  6. associate's degree
  7. bachelor's degree
  8. master's degree (non-professional)
  9. professional degree (MBA, MD, JD, etc.)
  10. Ph.D.

\newpage


# References


