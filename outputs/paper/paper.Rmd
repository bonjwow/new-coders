---
title: "The Impact of Educational Expenses of New Coders"
subtitle: "The Factors Affecting Education Costs of Beginner Programmers to Learn to Code aside from University Tuition"
author: "Bongju Yoo"
thanks: "https://github.com/bonjwow/new-coders"
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: ""
output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(janitor)
library(stargazer)
library(psych)
# install.packages("here")
library(here)
# install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
# install.packages("QuantPsyc")
library(QuantPsyc)
library(car)
library(kableExtra)
```


```{r, include=FALSE}
# Source: https://stefaneng.github.io/apa_correlation_table/
# install.packages("Hmisc")
library(Hmisc)

apply_if <- function(mat, p, f) {
  # Fill NA with FALSE
  p[is.na(p)] <- FALSE
  mat[p] <- f(mat[p])
  mat
}

apaCorr <- function(mat, corrtype = "pearson") {
  matCorr <- mat
  if (class(matCorr) != "rcorr") {
    matCorr <- rcorr(mat, type = corrtype)
  }
  # Add one star for each p < 0.05, 0.01, 0.001
  stars <- apply_if(round(matCorr$r, 2), matCorr$P < 0.05, function(x) paste0(x, "*"))
  stars <- apply_if(stars, matCorr$P < 0.01, function(x) paste0(x, "*"))
  stars <- apply_if(stars, matCorr$P < 0.001, function(x) paste0(x, "*"))
  # Put - on diagonal and blank on upper diagonal
  stars[upper.tri(stars, diag = T)] <- "-"
  stars[upper.tri(stars, diag = F)] <- ""
  n <- length(stars[1,])
  colnames(stars) <- 1:n
  # Remove _ and convert to title case
  row.names(stars) <- tools::toTitleCase(sapply(row.names(stars), gsub, pattern="_", replacement = " "))
  # Add index number to row names
  row.names(stars) <- paste(paste0(1:n,"."), row.names(stars))
  stars
}
```


# Introduction

With an increasing interest in computer programming and the growth of coding related jobs, more and more people invest their time and money in taking programming courses [@citeHughes]. In this paper, I examine some of the factors that may affect educational expenses of new coders or entry-level programmers aside from university tuition, mainly focusing on the level of education, the size of city, commuting time, income, and learning time. Approximately 20,000 beginner programmers had participated in the survey [@citeSurvey2017].

Firstly, I processed correlation analysis to identify the relationship between each independent variable and dependent variable (education costs) pair and found that all the pairs have a positive linear relationship except for commuting time. Secondly, I used multiple linear regression to determine the statistical significance of the factors. The result of the analysis shows that commuting time is not statistically significant. Interestingly, education background is also not statistically significant associated with educational expenses of beginner programmers. Other variables, such as the size of city, income, and learning time, have a statistical significance on the costs.

This analysis shows that beginner programmers living in small towns and/or low-income households are likely to spend less money on learning to code aside from university tuition than those who live in bigger cities and/or high-income households. It is assumed that there is still a lack of educational resources for new coders living in rural areas and/or low-income households. Free online learning platforms such as freeCodeCamp can be a great resource of education for the coders.

The paper is organized as follows. The Data section describes features of the original survey data and how the data is preprocessed. The Model section explains the multiple linear regression model used to assess the association between each factor and educational costs of beginner programmers, and evaluates the model using its residual standard error, multiple R-squared, and F-statistic. The Results section summarises the results of the regression model and model evaluation processes in the Model section. Lastly, the Discussion section discusses the findings and potential limitations of the paper, and suggests directions for future research related to this data.

# Data

```{r table1, echo=FALSE, warning=FALSE, message=FALSE}
#### Get data ####
dfNewCoders <- 
  readr::read_csv(here::here("inputs/data/clean_new-coders.csv"))

```
## Data Collection
The survey was run by `freeCodeCamp`, a non-profit organization that helps people learn to code through their free online courses and build a network for their alumni [@citeAbout]. The purpose of the survey was to examine how the users learn to code [@cite2017Survey]. The survey was conducted over the internet, and the survey respondents were limited to those persons with less than 5 years learning programming; all the respondents were asked whether they have already been coding for more than 5 years or not before starting the survey [@cite2017Survey]. The survey is composed of 48 questions and takes about five minutes to complete, and the survey results are under the Open Data Common License, which can be freely distributed through the organization’ GitHub repository [@cite2017Survey].^[https://github.com/freeCodeCamp/2017-new-coder-survey]

## Description of Dataset
The original dataset used for this paper is obtained from `freeCodeCamp`’s GitHub page. The format of the dataset is a comma-separated values (CSV) file, which contains 136 columns and 18,175 observations. I selected 8 variables and cleaned the data using the R programming language [@citeR] and the `tidyverse` package [@cityTidyverse] and the `dplyr` package [@cityDplyr]. The selected variables are: Age, CityPopulation, CommuteTime, Gender,  Income, MoneyForLearning, MonthsProgramming, and SchoolDegree. The CityPopulation is the estimated number of city population of the recipient; the question asked was "About how many people live in your city?”, and there are three options to choose: “less than 100,000”, "between 100,000 and 1 million", and "more than 1 million". Since these answers were coded as strings, I converted them into numeric variables using the `recode` function of `dplyr` [@cityDplyr]. The answers for CommuteTime and SchoolDegree were also coded as strings in the original dataset, so I applied the same data cleaning process to the variables as I did with CityPopulation. Also, I omitted observations which have a missing value. Table \@ref(tab:tabViewDataset) is a partial view of the cleaned dataset after the preprocessing.


```{r tabViewDataset, echo=FALSE, warning=FALSE, message=FALSE}
#### A partial view of the cleaned dataset ####
tabCleanedData <-
  dfNewCoders[1:10,] %>%
  mutate_all(linebreak) %>%
  kable(caption = "A partial view of the cleaned dataset",
      booktabs = T,
      escape = F,
      "latex",
      col.names = linebreak(c("Gender", 
                              "Age",
                              "City\nPopulation",
                              "Commute\nTime",
                              "Income",
                              "Months\nProgramming",
                              "School\nDegree",
                              "Money For\nLearning"), align = "c")) %>%
  kable_classic(full_width = F)

tabCleanedData

```

Table \@ref(tab:tabDescrStat) is



```{r tabDescrStat, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
#### Descriptive statistics ####

stargazer::stargazer(data.frame(dfNewCoders), 
                     type = "latex", 
                     title = "Descriptive statistics for the cleaned dataset", 
                     header = FALSE,
                     single.row = TRUE)



```

## Methodology and Data Collection




```{r, echo=FALSE, warning=FALSE, message=FALSE}
#### Correlation analysis ####

### Print correlation coefficient
tblCorr <-
  dfNewCoders %>%
  as.matrix() %>%
  apaCorr() %>%
  data.frame() %>%
  kable(caption = "Correlation matrix for variables",
        booktabs = TRUE,
        col.names = c("1", "2", "3", "4", "5", "6", "7", "8")) %>%
  footnote(general = "* p < 0.05, ** p < 0.01, *** p < 0.001",
             general_title = "")

tblCorr

```



```{r, echo=FALSE, warning=FALSE, message=FALSE}
### Print correlation matrix
PerformanceAnalytics::chart.Correlation(dfNewCoders, 
                                        histogram = TRUE, 
                                        pch = 19)
```


# Model
```{r, include=FALSE}
#### TODO's ####
# Write out what each variable is
# Weaknesses and next steps
```

## Model formulae

This equation \@ref(eq:linRegModel1) is a model formula for the regression model 1.

\begin{equation}
(\#eq:linRegModel1)
MoneyForLearning \sim \beta_0 + \beta_1 \times CityPopulation + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \epsilon
\end{equation}

This equation \@ref(eq:linRegModel2) is a model formula for the regression model 2.

\begin{equation}
(\#eq:linRegModel2)
MoneyForLearning \sim \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5 + \epsilon
\end{equation}

This equation \@ref(eq:linRegModel3) is a model formula for the regression model 3.

\begin{equation}
(\#eq:linRegModel3)
MoneyForLearning \sim \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_4 + \beta_5X_5 + \beta_6X_6 + \epsilon
\end{equation}



```{r, echo=FALSE, warning=FALSE, message=FALSE, results='asis'}
#### Multiple regression analysis ####
multiReg1 <- lm(formula = 
               MoneyForLearning ~ 
               CityPopulation +
               # CommuteTime +
               Income +
               MonthsProgramming +
               SchoolDegree, 
               data = dfNewCoders)

multiReg2 <- lm(formula = 
               MoneyForLearning ~ 
               Age +
               CityPopulation +
               # CommuteTime +
               Income +
               MonthsProgramming +
               SchoolDegree, 
               data = dfNewCoders)

multiReg3 <- lm(formula = 
               MoneyForLearning ~ 
               Gender +
               Age +
               CityPopulation +
               # CommuteTime +
               Income +
               MonthsProgramming +
               SchoolDegree, 
               data = dfNewCoders)


# summary(multiReg3)
stargazer::stargazer(multiReg1, multiReg2, multiReg3, 
          title = "Result of regression model", 
          header = FALSE,
          type = "latex")
```

## Model validation with RMSE

```{r}
#### RMSE with custom function ####
# Source: https://stackoverflow.com/questions/26237688/rmse-root-mean-square-deviation-calculation-in-r

RMSE <- function(error) {
  sqrt(mean(error^2)) 
}

RMSE(multiReg1$residuals)
RMSE(multiReg2$residuals)
RMSE(multiReg3$residuals)

# Model3 has the lowest RMSE among the 3 models => Model3 is the best!

```





```{r}
#### Beta ####
QuantPsyc::lm.beta(multiReg3)
```



```{r}
#### Check multicollinearity with Variance Inflation Factor (VIF) & Tolerance ####

### VIF
car::vif(multiReg3)
### Tolerance
1/car::vif(multiReg3)
```


# Results

# Discussion
```{r, include=FALSE}
#### TODO's ####
# Write at least 2.5 pages for the Discussion section
# Weaknesses and next steps
```


\newpage

# Appendix {-}

\newpage


# References


